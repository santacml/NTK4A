{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will calculate the NTK of a Bidirectional RNN in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T20:58:21.854150Z",
     "start_time": "2020-06-17T20:58:20.614135Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T20:58:21.860504Z",
     "start_time": "2020-06-17T20:58:21.855846Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils import clone_grads, paramdot, VErf3, VDerErf3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider the RNN with forward and backward propagation defined by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN.png](images/RNN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN_sampling.png](images/RNN_sampling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Finite-Width Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T20:58:21.896596Z",
     "start_time": "2020-06-17T20:58:21.862530Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class BidirectionalRNNClassifier(nn.Module):\n",
    "    def __init__(self, indim, statedim, outdim=1, nonlin=torch.erf, varu=1, varw=1, varb=0, varv=1,\n",
    "                avgpool=False, debug=False):\n",
    "        super().__init__()\n",
    "        self.varu = varu\n",
    "        self.varw = varw\n",
    "        self.varb = varb\n",
    "        self.varv = varv\n",
    "        self.nonlin = nonlin\n",
    "        self.avgpool = avgpool\n",
    "        self.debug = debug\n",
    "        self.W = nn.Parameter(torch.randn(statedim, statedim))\n",
    "        self.U = nn.Parameter(torch.randn(indim, statedim))\n",
    "        self.b = nn.Parameter(torch.randn(statedim))\n",
    "        self.v = nn.Parameter(torch.randn(statedim*2, outdim))\n",
    "        self.randomize()\n",
    "        \n",
    "    def forward(self, inp, initstate=0):\n",
    "        '''\n",
    "        Input:\n",
    "            inp: (batchsize, seqlen, indim)\n",
    "        Output:\n",
    "            out: (batchsize, outdim)\n",
    "        '''\n",
    "        indim = self.U.shape[0]\n",
    "        statedim = self.U.shape[1]\n",
    "        embed = torch.einsum(\n",
    "            'ijk,kl->ijl', inp, self.U) / np.sqrt(indim) + self.b\n",
    "        seqlen = inp.shape[1]\n",
    "        statef = initstate\n",
    "        stateb = initstate\n",
    "        self._states = []\n",
    "        self.hs = []\n",
    "        for i in range(seqlen):\n",
    "            hf = embed[:, i] + statef\n",
    "            statef = self.nonlin(hf)\n",
    "            hb = embed[:, seqlen-1-i] + stateb\n",
    "            stateb = self.nonlin(hb)\n",
    "\n",
    "            self._states.append(torch.cat((statef, stateb),1))\n",
    "            if self.debug:\n",
    "                state.retain_grad()\n",
    "            # _states[i] = s^{i+1}\n",
    "                self.hs.append(h)\n",
    "            \n",
    "            if i < seqlen - 1:\n",
    "                statef = statef @ self.W / np.sqrt(statedim)\n",
    "                stateb = stateb @ self.W / np.sqrt(statedim)\n",
    "            else:\n",
    "                if self.avgpool:\n",
    "                    meanstate = sum(self._states) / len(self._states)\n",
    "                    return meanstate @ self.v / np.sqrt(statedim)\n",
    "                else:\n",
    "                    return torch.cat((statef, stateb),1) @ self.v / np.sqrt(statedim)\n",
    "        \n",
    "    def randomize(self, varu=None, varw=None, varb=None, varv=None):\n",
    "        varu = varu or self.varu\n",
    "        varw = varw or self.varw\n",
    "        varb = varb or self.varb\n",
    "        varv = varv or self.varv\n",
    "        with torch.no_grad():\n",
    "            self.W.normal_(std=np.sqrt(varw))\n",
    "            self.U.normal_(std=np.sqrt(varu))\n",
    "            self.v.normal_(std=np.sqrt(varv))\n",
    "            if varb > 0:\n",
    "                self.b.normal_(std=np.sqrt(varb))\n",
    "            else:\n",
    "                self.b.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T20:58:21.923581Z",
     "start_time": "2020-06-17T20:58:21.898357Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def simrnn_ntk(inputseqs, width, phi, varw=1, varu=1, varb=0, varv=1, seed=None, avgpool=False, debug=False):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    indim = inputseqs[0].shape[1]\n",
    "    rnn = BidirectionalRNNClassifier(indim, width, nonlin=phi,\n",
    "                              varw=varw, varu=varu, varb=varb, varv=varv,\n",
    "                              avgpool=avgpool, debug=debug)\n",
    "    grads = []\n",
    "    sgrads = []\n",
    "    states = []\n",
    "    hs = []\n",
    "    for seq in inputseqs:\n",
    "        out = rnn(seq.unsqueeze(0))\n",
    "        out.backward()\n",
    "        grads.append(clone_grads(rnn))\n",
    "        if debug:\n",
    "            # shape time x dim_s\n",
    "            sgrads.append(clone_sgrads(rnn))\n",
    "            states.append(clone_states(rnn))\n",
    "            hs.append(clone_hs(rnn))\n",
    "        rnn.zero_grad()\n",
    "        \n",
    "    batchsize = len(inputseqs)\n",
    "    \n",
    "    ntk = torch.zeros(batchsize, batchsize)\n",
    "    for i in range(batchsize):\n",
    "        for j in range(0, i+1):\n",
    "            ntk[i, j] = ntk[j, i] = paramdot(grads[i], grads[j])\n",
    "    \n",
    "    if not debug:\n",
    "        return dict(ntk=ntk)\n",
    "    \n",
    "    dscov = torch.zeros(batchsize, batchsize).tolist()\n",
    "    scov = torch.zeros(batchsize, batchsize).tolist()\n",
    "    hhcov = torch.zeros(batchsize, batchsize).tolist()\n",
    "    for i in range(batchsize):\n",
    "        for j in range(0, i+1):\n",
    "            dscov[i][j] = (sgrads[i] @ sgrads[j].T).numpy()\n",
    "            dscov[j][i] = dscov[i][j].T\n",
    "            scov[i][j] = (states[i] @ states[j].T).numpy() / states[i].shape[-1]\n",
    "            scov[j][i] = scov[i][j].T\n",
    "            hhcov[i][j] = (hs[i] @ hs[j].T).numpy() / hs[i].shape[-1]\n",
    "            hhcov[j][i] = hhcov[i][j].T\n",
    "    return dict(ntk=ntk, dscov=np.block(dscov), scov=np.block(scov), hhcov=np.block(hhcov))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Infinite-Width NTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Consider two input sequences $\\xi = \\{\\xi^1, \\ldots, x^T \\in \\mathbb R^d\\}$ and $\\bar \\xi = \\{\\bar \\xi^1, \\ldots, \\bar \\xi^{\\bar T} \\in \\mathbb R^d \\}$.\n",
    "We abbreviate $s^t = s^t(\\xi)$ and $\\bar s^t = s^t(\\bar \\xi)$, and likewise for other vectors $g^t, u^t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The infinite-width NTK then depends on the inner products\n",
    "$$C^{s^t, \\bar s^r} = \\lim_{n\\to\\infty} n^{-1} s^{t\\top} \\bar s^r,\\quad D^{g^t, \\bar g^r} = \\lim_{n\\to\\infty} n^{-1} dg^{t\\top} d\\bar g^r$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![RNN_forward_backward.png](images/RNN_forward_backward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![RNN_LastState.png](images/RNN_LastState.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![RNN_AvgPool.png](images/RNN_AvgPool.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![RNN_NTK.png](images/RNN_NTK.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T20:58:21.960684Z",
     "start_time": "2020-06-17T20:58:21.925334Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def rnnntk_batch(ingram, Vphi3, Vderphi3,\n",
    "    varw=1, varu=1, varb=0, varv=1, avgpool=False):\n",
    "    ''' Compute the RNN-NTK over a batch of sequences of the same length\n",
    "    Inputs:\n",
    "        `ingram`: dimension-normalized Gram matrix between all tokens across all input sequences\n",
    "            of shape [batchsize, batchsize, seqlen, seqlen]\n",
    "        `Vphi3`: V-transform of nonlin that takes in 3 input arrays (cov, var1, var2)\n",
    "        `Vderphi3`: V-transform of nonlin derivative that takes in 3 input arrays (cov, var1, var2)\n",
    "        `varw`: variance of state-to-state weights\n",
    "        `varu`: variance of input-to-state weights\n",
    "        `varb`: variance of biases\n",
    "        `varv`: variance of output weights\n",
    "        `avgpool`: if True, output is the average of all states multiplied by output weights.\n",
    "            Otherwise, output is just the last state multiplied by output weigths.\n",
    "    Outputs:\n",
    "        a dictionary of kernels\n",
    "        output['ntk'] gives the NTK\n",
    "        '''\n",
    "\n",
    "\n",
    "    ingram_b = np.zeros(ingram.shape)\n",
    "    ingram_b[0,0,...]= ingram[0,0,...][::-1, ::-1].T\n",
    "    ingram_b[0,1,...]= ingram[1,0,...][::-1, ::-1].T\n",
    "    ingram_b[1,0,...]= ingram[0,1,...][::-1, ::-1].T\n",
    "    ingram_b[1,1,...]= ingram[1,1,...][::-1, ::-1].T\n",
    "\n",
    "    # ingram_b = ingram\n",
    "\n",
    "    seqlen = ingram.shape[-1]\n",
    "    batchsize = ingram.shape[0]\n",
    "    # hcov[ia, jb] = < h^(i+2,a), h^(j+2,b) >\n",
    "    hcov_f = np.zeros(ingram.shape)\n",
    "    hcov_b = np.zeros(ingram.shape)\n",
    "    # hhcov[ia, jb] = < \\tilde h^(i+1,a), \\tilde h^(j+1,b) >\n",
    "    hhcov_f = np.zeros(ingram.shape)\n",
    "    hhcov_f[..., 0, :] = varu * ingram[..., 0, :] + varb\n",
    "    hhcov_f[..., :, 0] = varu * ingram[..., :, 0] + varb\n",
    "\n",
    "    hhcov_b = np.zeros(ingram.shape)\n",
    "    hhcov_b[..., 0, :] = varu * ingram_b[..., 0, :] + varb\n",
    "    hhcov_b[..., :, 0] = varu * ingram_b[..., :, 0] + varb\n",
    "    # fill in zeroed entries\n",
    "    def reflect(t):\n",
    "        return np.where(t==0, np.moveaxis(t, [0, 2], [1, 3]), t)\n",
    "    def hhcov_f_prep(i=None, b=0):\n",
    "        if i is None:\n",
    "            d = np.einsum('aaii->ai', hhcov_f)\n",
    "            return np.broadcast_arrays(\n",
    "                hhcov_f,\n",
    "                d.reshape(batchsize, 1, seqlen, 1),\n",
    "                d.reshape(1, batchsize, 1, seqlen)\n",
    "            )\n",
    "        return np.broadcast_arrays(\n",
    "            hhcov_f[..., i, b:i+1],\n",
    "            np.diag(hhcov_f[..., i, i]).reshape(batchsize, 1, 1),\n",
    "            np.einsum('aaii->ai', hhcov_f[..., b:i+1, b:i+1]).reshape(1, batchsize, i+1-b)\n",
    "        )\n",
    "        return np.where(t==0, np.moveaxis(t, [0, 2], [1, 3]), t)\n",
    "    def hhcov_b_prep(i=None, b=0):\n",
    "        if i is None:\n",
    "            d = np.einsum('aaii->ai', hhcov_b)\n",
    "            return np.broadcast_arrays(\n",
    "                hhcov_b,\n",
    "                d.reshape(batchsize, 1, seqlen, 1),\n",
    "                d.reshape(1, batchsize, 1, seqlen)\n",
    "            )\n",
    "        return np.broadcast_arrays(\n",
    "            hhcov_b[..., i, b:i+1],\n",
    "            np.diag(hhcov_b[..., i, i]).reshape(batchsize, 1, 1),\n",
    "            np.einsum('aaii->ai', hhcov_b[..., b:i+1, b:i+1]).reshape(1, batchsize, i+1-b)\n",
    "        )\n",
    "    def Vderphi(mat):\n",
    "        d = np.diag(mat)\n",
    "        return Vderphi3(mat, d.reshape(-1, 1), d.reshape(1, -1))\n",
    "    \n",
    "    for i in range(0, seqlen):\n",
    "        hcov_f[..., i, :i+1] = varw * Vphi3(*hhcov_f_prep(i))\n",
    "        if i < seqlen - 1:\n",
    "            hhcov_f[..., i+1, 1:i+2] = hcov_f[..., i, :i+1] + varu * ingram[..., i+1, 1:i+2] + varb\n",
    "\n",
    "        hcov_b[..., i, :i+1] = varw * Vphi3(*hhcov_b_prep(i))\n",
    "        if i < seqlen - 1:\n",
    "            hhcov_b[..., i+1, 1:i+2] = hcov_b[..., i, :i+1] + varu * ingram_b[..., i+1, 1:i+2] + varb\n",
    "\n",
    "        \n",
    "    hhcov_f = reflect(hhcov_f)\n",
    "    hcov_f = reflect(hcov_f)\n",
    "    scov_f = varw**-1 * hcov_f\n",
    "\n",
    "    hhcov_b = reflect(hhcov_b)\n",
    "    hcov_b = reflect(hcov_b)\n",
    "    scov_b = varw**-1 * hcov_b\n",
    "\n",
    "    hhcov = (hhcov_f + hhcov_b ) /2\n",
    "    hcov = (hcov_f + hcov_b ) / 2\n",
    "    scov = varw**-1 * hcov\n",
    "    \n",
    "    if not avgpool:\n",
    "        dhcov_f = np.zeros([batchsize, batchsize, seqlen+1])\n",
    "        dhcov_f[..., -1] = varv\n",
    "        for i in range(seqlen-1, -1, -1):\n",
    "            dhcov_f[..., i] = varw * Vderphi(hhcov[..., i, i]) * dhcov_f[..., i+1]\n",
    "        dhcov_f /= varw\n",
    "        dhcov_f = dhcov_f[..., :-1]\n",
    "\n",
    "\n",
    "        dhcov_b = np.zeros([batchsize, batchsize, seqlen+1])\n",
    "        dhcov_b[..., -1] = varv\n",
    "        for i in range(seqlen-1, -1, -1):\n",
    "            dhcov_b[..., i] = varw * Vderphi(hhcov[..., i, i]) * dhcov_b[..., i+1]\n",
    "        dhcov_b /= varw\n",
    "        dhcov_b = dhcov_b[..., :-1]\n",
    "\n",
    "        \n",
    "        buf_f = np.einsum('abii->abi', ingram) + 1\n",
    "        buf_f[..., 1:] += np.einsum('abii->abi', scov[..., :-1, :-1])\n",
    "        ntk_f = np.einsum('abi,abi->ab', dhcov_f, buf_f)\n",
    "        ntk_f += scov[..., -1, -1]\n",
    "\n",
    "        \n",
    "        buf_b = np.einsum('abii->abi', ingram_b) + 1\n",
    "        buf_b[..., 1:] += np.einsum('abii->abi', scov[..., :-1, :-1])\n",
    "        ntk_b = np.einsum('abi,abi->ab', dhcov_b, buf_b)\n",
    "        ntk_b += scov[..., -1, -1]\n",
    "        return (ntk_f + ntk_b ) \n",
    "\n",
    "    0/0\n",
    "\n",
    "    # dscov[ia, jb] = <ds^(i+1,a), ds^(j+1,b)>\n",
    "    dscov = np.zeros(ingram.shape)\n",
    "    dscov[..., :, -1] = dscov[..., -1, :] = varv\n",
    "    for i in range(seqlen-1, 0, -1):\n",
    "        dscov[..., i-1, :i] = varw * Vderphi3(*hhcov_prep(i, 1)) * dscov[..., i, 1:i+1] + varv\n",
    "    dscov = reflect(dscov)\n",
    "\n",
    "    # dhcov[ia, jb] = <d\\tilde h^(i+1,a), d\\tilde h^(j+1,b)>\n",
    "    dhcov = Vderphi3(*hhcov_prep()) * dscov\n",
    "    \n",
    "    buf = ingram + 1\n",
    "    buf[..., 1:, 1:] += scov[..., :-1, :-1]\n",
    "    ntk = np.einsum('abij,abij->ab', dhcov, buf)\n",
    "    ntk += np.sum(scov, axis=(-1, -2))\n",
    "    return dict(ntk=ntk / seqlen**2, dscov=dscov, scov=scov, hcov=hcov_f, hhcov=hhcov_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Theory vs Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We verify that as width increases, the empirical finite-width NTK converges to the theoretical infinite-width NTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T20:58:22.976287Z",
     "start_time": "2020-06-17T20:58:21.995796Z"
    },
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We treat the first 5 pixels (3 channels each) as the \"tokens\" in a sequence of 5. We use the first 2 images from CIFAR10 for our 2 sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T20:58:32.817691Z",
     "start_time": "2020-06-17T20:58:22.978453Z"
    },
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "M = 2\n",
    "T = 5\n",
    "\n",
    "inps = torch.stack([v[0].reshape(3, -1)[:, :T].T for i, v in enumerate(trainset)\n",
    "                 if i < M]).numpy()\n",
    "inpcov = np.einsum('ais,bjs->aibj', inps, inps) / inps.shape[-1]\n",
    "inpcov = np.moveaxis(inpcov, 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T20:58:32.822207Z",
     "start_time": "2020-06-17T20:58:32.819402Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "varw = 1\n",
    "varu = 2\n",
    "varb = 0.2\n",
    "varv = 1\n",
    "avgpool = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T20:58:32.841304Z",
     "start_time": "2020-06-17T20:58:32.823918Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def VErf(cov):\n",
    "    '''\n",
    "    Computes E[erf(z) erf(z)^T | z ~ N(0, `cov`)]\n",
    "    where z is a multivariate Gaussian with mean 0 and covariance `cov`\n",
    "\n",
    "    Inputs:\n",
    "        `cov`: An array where the last 2 dimensions contain covariance matrix of z (and the first dimensions are \"batch\" dimensions)\n",
    "    Output:\n",
    "        a numpy array of the same shape as `cov` that equals the\n",
    "        expectation above in the last 2 dimensions.\n",
    "    '''\n",
    "    ll = list(range(cov.shape[-1]))\n",
    "    d = np.sqrt(cov[..., ll, ll] + 0.5)\n",
    "\n",
    "    c = d[..., None]**(-1) * cov * d[..., None, :]**(-1)\n",
    "    return 2./np.pi * np.arcsin(np.clip(c, -1, 1))\n",
    "\n",
    "\n",
    "def VDerErf(cov):\n",
    "    '''\n",
    "    Computes E[erf'(z) erf'(z)^T | z ~ N(0, `cov`)]\n",
    "    where erf' is the derivative of erf and\n",
    "    z is a multivariate Gaussian with mean 0 and covariance `cov`\n",
    "\n",
    "    Inputs:\n",
    "        `cov`: An array where the last 2 dimensions contain covariance matrix of z (and the first dimensions are \"batch\" dimensions)\n",
    "    Output:\n",
    "        a numpy array of the same shape as `cov` that equals the\n",
    "        expectation above in the last 2 dimensions.\n",
    "    '''\n",
    "    ll = list(range(cov.shape[-1]))\n",
    "    d = np.sqrt(cov[..., ll, ll])\n",
    "    dd = 1 + 2 * d\n",
    "    return 4/np.pi * (dd[..., None] * dd[..., None, :] - 4 * cov**2)**(-1./2)\n",
    "\n",
    "thcov = rnnntk_batch(inpcov, VErf3, VDerErf3, varw, varu, varb, varv, avgpool=avgpool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T20:58:32.863597Z",
     "start_time": "2020-06-17T20:58:32.842856Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def NTK_theory_vs_sim(inpseqs, infntk, varw, varu, varb, avgpool,\n",
    "                      nonlin=torch.erf,\n",
    "                      log2widthmin=6, log2widthmax=15, nseeds=10):\n",
    "    if isinstance(inpseqs, np.ndarray):\n",
    "        inpseqs = torch.from_numpy(inpseqs).float()\n",
    "    widths = 2**np.arange(log2widthmin, log2widthmax)\n",
    "    mysimcovs = {}\n",
    "    for width in tqdm(widths):\n",
    "        mysimcovs[width] = np.stack([\n",
    "            simrnn_ntk(inpseqs, width,\n",
    "                       nonlin, varw, varu, varb,\n",
    "                       seed=seed, avgpool=avgpool, debug=False)['ntk']\n",
    "            for seed in range(nseeds)])\n",
    "    frobs = []\n",
    "    infntknorm = np.linalg.norm(infntk)\n",
    "    for width in widths:\n",
    "        _frobs = np.sum((mysimcovs[width] - infntk)**2,\n",
    "                        axis=(1, 2)) / infntknorm**2\n",
    "        for f in _frobs:\n",
    "            frobs.append(dict(\n",
    "                relfrob=np.sqrt(f),\n",
    "                width=width\n",
    "            ))\n",
    "    return pd.DataFrame(frobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We measured the relative Frobenius norm of the finite-width deviation = $\\|\\Theta - \\mathring \\Theta\\|_F / \\|\\mathring \\Theta\\|_F$, where $\\Theta$ and $\\mathring \\Theta$ are resp. the finite (empirical) and infinite-width (theoretical) NTKs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T21:05:30.779105Z",
     "start_time": "2020-06-17T20:58:32.865163Z"
    },
    "hidden": true,
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "frob_df = NTK_theory_vs_sim(inps, thcov, varw, varu, varb,\n",
    "                           avgpool=True, log2widthmax=14, nseeds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T21:05:32.092838Z",
     "start_time": "2020-06-17T21:05:30.780738Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sns.lineplot(data=frob_df, x='width', y='relfrob')\n",
    "widths = frob_df.width.unique()\n",
    "plt.plot(widths, np.array(widths, dtype='float')**-0.5, '--', label=u'${width}^{-1/2}$')\n",
    "plt.ylabel(u'Relative Frob. Norm')\n",
    "plt.loglog()\n",
    "plt.legend()\n",
    "_ = plt.title(u'Bidirectional RNN NTK\\nDeviation from theory drops like $width^{-1/2}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T21:05:32.099698Z",
     "start_time": "2020-06-17T21:05:32.094391Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "frob_df.to_pickle('birnn_ap_ntk.frob')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('netsor': conda)",
   "language": "python",
   "name": "python38364bitnetsorconda90413646b69740418857320fb86a86c0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}